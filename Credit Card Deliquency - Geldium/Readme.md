# ğŸ’³ğŸ‘©ğŸ»â€ğŸ’»Customer Delinquency Risk Prediction â€“ Geldium (Gen-AI Data Analytics)
<img width="698" height="392" alt="image" src="https://github.com/user-attachments/assets/abd0e8d0-00ce-44a8-a0b9-00dbbedf6f3c" />


This repository contains my end-to-end **Machine Learning Project** from the
ğŸ“½ï¸**Tata/Forage â€œGen-AI Data Analyticsâ€ virtual experience**.

The goal is to help a fictional fintech company **Geldium** identify **high-risk
customers** who are likely to become **delinquent on their payments**, and to
understand *why* they are risky so that business teams can design proactive
interventions.

>ğŸ‘¾ **Tech stack:** Python, Pandas, NumPy, Matplotlib, Seaborn, Scikit-learn  

> ğŸ“**Main file:** `Forage.ipynb`


---

## ğŸ§© Problem Statement

Geldium provides consumer credit products and wants to:

1. **Predict** which customers are likely to miss several upcoming payments.
2. **Explain** the drivers of delinquency in a way that is transparent and
   regulator-friendly.
3. **Balance** model performance with fairness, interpretability and ease of
   deployment.

Using a historical payment dataset, the task is to build and compare ML models
that classify customers into:

- `High_Risk_Customer` â€“ customers with **3+ recent late/missed payments**  
- `Low_Risk_Customer` â€“ everyone else

and then recommend the best model & pipeline for production.


---

## ğŸ“‚ Dataset

Synthetic customer-level dataset (`Forage_Deliquency.xlsx`, provided by Forage)
containing **several thousand anonymised customers** with features such as:

- `Age`
- `Income`
- `Credit_Score`
- `Credit_Utilization`
- `Loan_Balance`
- `Missed_Payments` (historical)
- `Late_Missed_Count` (recent 6-month window)
- `Employment_Status` (encoded into `Employment_Status_Encoded`)
- Derived features: `Debt_to_Income_Ratio`, `Account_Tenure`, etc.

---
## ğŸ” Exploratory Data Analysis (EDA)

The notebook first focuses on **understanding the data and its quality**.

### 1ï¸âƒ£ Structure & Data Quality

- âœ… Inspected **shape, dtypes, summary statistics and value ranges**.  
- ğŸ” Identified **missing values** in numeric features such as `Income`, `Loan_Balance` and occasionally `Credit_Score`.  
- ğŸ“Š Created a **missing-data table** with counts and percentages to decide how each column should be treated.

### 2ï¸âƒ£ Handling Missing Data

- ğŸ§¹ For `Income`, `Loan_Balance`, `Credit_Score` â†’ **imputed with median values** to keep the distribution robust to outliers.  
- âœ”ï¸ Verified that the total number of missing values after treatment is **zero**.

### 3ï¸âƒ£ Behaviour & Risk Patterns

Multiple visualisations are created to understand delinquency drivers:
- ğŸ“ˆ **Distribution of** Late_Missed_Count â€“ shows how many customers are repeatedly late.
- ğŸ“‰ **Credit Score vs Late Payments** â€“ lower scores cluster around higher late-payment counts.
- ğŸ“Š **Credit Utilization vs Late Payments** â€“ high utilisation is strongly associated with risk.
- ğŸ” **Total Missed_Payments vs recent late payments** â€“ shows that historical behaviour predicts future behaviour.

These plots confirm intuitive risk patterns:
- âš ï¸ Customers with **lower credit scores, higher utilisation** and **more historical** missed payments are more likely to be **high risk**.
- ğŸ’° **Income** and **Debt-to-Income ratio** also show clear separation between low-risk and high-risk segments.

---

## ğŸ§® Feature Engineering & Selection

Key steps to prepare data for modelling:
### ğŸ¯ Target Creation
<img width="697" height="58" alt="image" src="https://github.com/user-attachments/assets/74b91d82-9c84-499e-8a0c-d44ed98c884a" />

Customers with Late_Missed_Count >= 3 are labelled as **High Risk (1)**, others as **Low Risk (0)**.

### ğŸ”¡ Encoding Categorical Features
<img width="570" height="295" alt="image" src="https://github.com/user-attachments/assets/558438d1-699e-4a80-b84e-baf355b40895" />
This converts Employment_Status into a numeric feature for modelling.

### ğŸ§± Feature Candidates
<img width="607" height="165" alt="image" src="https://github.com/user-attachments/assets/1049e745-a033-46be-a005-47dc64fdfba4" />
These features are used as inputs to the machine learning models.

### âš™ï¸ Standardisation & Train/Test Split
- ğŸ“ Numerical features are **standardised** using StandardScaler.
- âœ‚ï¸ Data is split into **training** and **test sets** with train_test_split to evaluate generalisation.

### â­ Feature Importance (ANOVA F-test)
- Used SelectKBest(f_classif) to **rank all candidate features** and select the **top 5 most predictive variables**.
- A **correlation heatmap** is plotted to check **multicollinearity** and relationships between features.

---

## ğŸ¤– Model Development

Two main models are trained and compared:
### 1ï¸âƒ£ Logistic Regression â€“ Simple & Interpretable
- âš–ï¸ class_weight="balanced" to handle class imbalance.
- ğŸ“ Features are **standardised**.
- ğŸ”§ Hyperparameters tuned using GridSearchCV:
    - Regularisation strength
    - Penalty
    - Solver
    - Class weights

### 2ï¸âƒ£ Random Forest Classifier â€“ Non-linear Baseline
- ğŸŒ³ Able to capture non-linear relationships and feature interactions.
- ğŸ”§ Tuned hyperparameters include:
      - Number of trees (n_estimators)
      - Maximum depth (max_depth)
      - Minimum samples split (min_samples_split)
      - Minimum samples leaf (min_samples_leaf)
      - Class weights

### ğŸ“ Evaluation Metrics
Both models are evaluated on the test set using:
- âœ… Accuracy
- ğŸ¯ Precision
- ğŸ“‰ Recall
- âš–ï¸ F1 Score
- ğŸ“ˆ ROCâ€“AUC Score
- â± Training time

### ğŸ§  Feature Importance Analysis
- ğŸ§® **Logistic Regression coefficients** â€“ show the **direction and strength** of impact for each feature.
- ğŸŒ² **Random Forest feature importances** â€“ show the **relative contribution** of each feature in a non-linear model.

---
## ğŸ“Š Results & Insights
Exact numbers may vary with different random splits / hyperparameters, but the overall pattern is:

### âœ… Overall Performance
- Both models achieve **strong classification performance**, with **Logistic Regression** and **Random Forest** showing very similar **AUC and F1 scores**.
- **Random Forest**:
  - Slightly better on **pure predictive power**, especially for complex interactions.
- **Logistic Regression**:
  - **Fully transparent** and **faster** to train and infer.

### ğŸ†š Model Comparison
The notebook includes:
- ğŸ“‹ A **comparison table** summarising **metrics** for both models.
- âš–ï¸ A **pros & cons** matrix comparing Logistic Regression vs Random Forest across:
  - Interpretability
  - Performance
  - Speed
  - Deployment complexity
  - Regulatory compliance
  - Maintenance
- ğŸ“Š Visual comparison of **feature importance** between both models.
---

## ğŸ† Selected Model
Final Choice: âœ… Optimised Logistic Regression

### ğŸ’¡ Why Logistic Regression?
- ğŸ“ˆ **Strong, stable performance** with competitive AUC/F1 compared to Random Forest.
- ğŸ” Transparent feature coefficients support **regulatory compliance** and **fair-lending requirements**.
- ğŸ—£ Easy to explain to non-technical stakeholders
  â†’ e.g., â€œthis feature increases risk by X%â€.
- âš™ï¸ Lightweight and simple to integrate into existing **risk engines** and **decision workflows**.
---

## ğŸ”„ Final Pipeline (Conceptual)

<img width="457" height="321" alt="image" src="https://github.com/user-attachments/assets/0f11c53d-de9d-4de8-a93b-427a12769037" />

---
## âš–ï¸ Fairness & Governance Considerations
The notebook also designs a **fairness monitoring framework** for future production use.

- **ğŸ‘¥ Protected Attributes to Monitor**
  - Age groups
  - Income bands
  - Credit profile tiers

- **ğŸ“ Fairness Metrics (Conceptual)**
  - âš–ï¸ Statistical parity â€“ selection rates across groups
  - ğŸš¨ Predictive equality â€“ false-positive rates
  - ğŸŸ° Equalised odds â€“ error rates across groups
  - ğŸ¯ Treatment equity â€“ precision by group

---

## ğŸ§¾ Key Takeaways
- Built a complete **credit-risk classification pipeline** from raw Excel data to cleaned dataset, EDA, feature engineering and model selection.
- Compared a **simple interpretable model (Logistic Regression)** with a more complex **Random Forest** baseline.
- Selected Logistic Regression as the **final model** due to its balance of performance, interpretability and regulatory alignment.
- Produced **artifacts** (cleaned data, text summary, plots) that can be used by risk, product and collections teams to understand and act on delinquency risk.
